{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LopqFgh8slxt"
   },
   "source": [
    "References: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJP86bjEss1g"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWK0fjB992BH",
    "outputId": "ea714a08-9bbf-4c96-afcb-de1859a84089",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U einops\n",
    "!pip install -q -U safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B7SWfLrtFpH"
   },
   "source": [
    "### bitsandbytes configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRhsJEAstiqf"
   },
   "source": [
    "The 4bit integration comes with 2 different quantization types: FP4 and NF4. The NF4 dtype stands for Normal Float 4 and is introduced in the QLoRA paper\n",
    "\n",
    "You can switch between these two dtype using bnb_4bit_quant_type from BitsAndBytesConfig. By default, the FP4 quantization is used.\n",
    "\n",
    "This saves more memory at no additional performance - from our empirical observations, this enables fine-tuning llama-13b model on an NVIDIA-T4 16GB with a sequence length of 1024, batch size of 1 and gradient accumulation steps of 4.\n",
    "\n",
    "To enable this feature, simply add `bnb_4bit_use_double_quant=True` when creating your quantization config!\n",
    "\n",
    "(text from HF colab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4tp_hhhu55l"
   },
   "source": [
    "We will used NF4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSN4W1F-tKy7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GdYFG4NsxWq"
   },
   "source": [
    "### Load model and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCbmMPQtoNyO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# My version with smaller chunks on safetensors for low RAM environments\n",
    "#model_id = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "#model_id = \"anakin87/zephyr-7b-alpha-sharded\"\n",
    "model_id = \"Trelis/Llama-2-7b-chat-hf-sharded-bf16\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "84a185a47e254158ae9f7f492ae02fa8",
      "d308ecc2e23e4a378e9cc7ff5211dc3c",
      "e10854f16f7c4e7faa88ca52c3b18fe1",
      "b581a351180f4a1ea9248d71c457e5dd",
      "64ab742910744170bd89317551c16983",
      "0635039fc1bf4e8385b610b2e821d337",
      "d8fdf0dbcf4c4f1c9fb6404849a3c1b0",
      "d7076616114547709f55a2b6dadd5b47",
      "b60bae131e7c4c598221192dd0bf9575",
      "632d7836000748c2a9180971b0bb16cb",
      "b9b752bca85d4c67b94acce7799eb3b8"
     ]
    },
    "id": "ewiOVKpZ-fnX",
    "outputId": "be42a970-93be-4ac8-8340-9bbb62b0eb19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzQmC98jAi2n",
    "outputId": "9993b099-c475-4218-e3b5-a313242beed4"
   },
   "outputs": [],
   "source": [
    "print(model_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30HVdqsI8f0O",
    "outputId": "2eed9a78-882c-471b-b0a0-3318d8f9c918"
   },
   "outputs": [],
   "source": [
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWjNo6kO-AP6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_4bit,\n",
    "        tokenizer=tokenizer,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        #max_length=296,\n",
    "        max_length=2048,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E72P0q8W2eJf"
   },
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NARv8UJ84Uy",
    "outputId": "83bcd005-e306-41e0-dffc-fd4a88305da8"
   },
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlauSfZlrpvO"
   },
   "source": [
    "### Use with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LFP9w_wAOal"
   },
   "outputs": [],
   "source": [
    "# Some error in colab. fix with\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOY1YDwIrtK_",
    "outputId": "7f8606bf-f62d-4a40-f8de-b375d1531fd6"
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpnYgcStAbNu"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9QZF9OEAmT0"
   },
   "source": [
    "Load local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0tanbAJAcoe"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNnrM3elwhZP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzEGR1y3Aj_1"
   },
   "source": [
    "Define Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_eOs_lNAhJD"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables= [\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p772TZxIApQA"
   },
   "source": [
    "Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n19TrTGkuDo6"
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPglltaaw2cq",
    "outputId": "87f648d2-0c6f-4058-ed7d-881c6d90e453"
   },
   "outputs": [],
   "source": [
    "!pip install \"weaviate-client==3.*\"\n",
    "!pip install sentence-transformers\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ye1eVR-cxGmL"
   },
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents.types import AgentType\n",
    "from langchain.agents import AgentExecutor, Tool,initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_A0yhwqtxtMF",
    "outputId": "0f7faade-8c47-47ca-b0de-e4cba74cfcbd"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9aAgElExx_X",
    "outputId": "75cb003c-b5ee-47c3-fb4b-8013efae9143"
   },
   "outputs": [],
   "source": [
    "!ls drive/MyDrive/'Colab Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeGvI0TKwmOq"
   },
   "outputs": [],
   "source": [
    "WEAVIATE_URL = \"https://ragtestarray-4gihzxpr.weaviate.network\"\n",
    "WEAVIATE_API_KEY = \"7E0Vf7POMdgUkpQfEHj5hPMpfUtPxNCNIisB\"\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxVFb8eSxIjC"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/kidney_cancer_stories_v2.txt\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "scAf--pcw7ED",
    "outputId": "b64eb6f6-a167-4ad0-a87c-266e886dff73"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DesCKOJ8yBt8"
   },
   "outputs": [],
   "source": [
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#model_kwargs = {\"device\": \"cuda\"} mps\n",
    "#model_kwargs = {\"device\": \"mps\"}\n",
    "model_kwargs = {}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name,\n",
    "  model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFfUbUKB0fzq"
   },
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN1I7QeZ_u6w"
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "# vectors_docs = []\n",
    "count = 0\n",
    "for index, d in data.iterrows():\n",
    "    try:\n",
    "        #list_docs = text_splitter.create_documents([d[\"text\"]])\n",
    "        #split = text_splitter.split_documents(list_docs)\n",
    "        try:\n",
    "            # base_docs = text_splitter.split_text(d['headline'] + d[\"text\"])\n",
    "            # list_docs = [d['title'] + '##' + base_docs[i] for i in range(0, len(base_docs))]\n",
    "            base_docs = text_splitter.split_text(d['Story'])\n",
    "            list_docs = [d['Name'] + '##' + base_docs[i] for i in range(0, len(base_docs))]\n",
    "            create_docs = text_splitter.create_documents(list_docs)\n",
    "            #split_docs = text_splitter.split_documents(list_docs)\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            list_docs = []\n",
    "            create_docs = []\n",
    "            split_docs = []\n",
    "            continue\n",
    "\n",
    "        # try:\n",
    "        #     vector_list = embeddings.embed_documents(list_docs)\n",
    "        # except:\n",
    "        #     vector_list = []\n",
    "        all_docs.extend(create_docs)\n",
    "        # vectors_docs.append(vector_list)\n",
    "        count = count + 1\n",
    "    except Exception as  e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W066WsJM_wx5"
   },
   "outputs": [],
   "source": [
    "vector_db = Weaviate.from_documents(\n",
    "    all_docs, embeddings, client=client, by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4T0xgRaAD2E",
    "outputId": "917c0d6c-915a-4272-9319-0c349e8552d4"
   },
   "outputs": [],
   "source": [
    "vector_db.similarity_search(\"What are side effects of kidney cancer?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymHgfWBLAaEg"
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vector_db.as_retriever(search_kwargs={\"k\": 4}), return_source_documents=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M29LgLE4CZeZ",
    "outputId": "9306c775-08e7-4189-e6bd-ec185852aba8"
   },
   "outputs": [],
   "source": [
    "response = qa_chain(\"What are side effects of kidney cancer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc-MYuQnCctC",
    "outputId": "aa56a402-392f-47b3-e784-e4aa9925edb5"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqbTmVIIFUW8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOrXDrjrEs49"
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', return_messages=True, output_key='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6e7OsF9E1I9"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "\"You are the XYZ bot.\"\n",
    "\"This is conversation with a human. Answer the questions you get based on the knowledge you have.\"\n",
    "\"If you don't know the answer, just say that you don't, don't try to make up an answer.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3bSB40dDucd"
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "        Tool(\n",
    "            name=\"doc_search_tool\",\n",
    "            func=qa_chain,\n",
    "            description=(\n",
    "               \"This tool is used to retrieve information from the knowledge base\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# agent = initialize_agent(\n",
    "#         agent = AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "#         tools=tools,\n",
    "#         llm=llm,\n",
    "#         memory=memory,\n",
    "#         return_source_documents=True,\n",
    "#         return_intermediate_steps=True,\n",
    "#         agent_kwargs={\"system_message\": system_message}\n",
    "#         )\n",
    "agent = initialize_agent(\n",
    "        agent = AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        # memory=memory,\n",
    "        return_source_documents=True,\n",
    "        return_intermediate_steps=True,\n",
    "        agent_kwargs={\"system_message\": system_message}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "id": "HWf_UbZEE2lU",
    "outputId": "be8f8763-ef39-4f5d-c0f5-779419ce6a6b"
   },
   "outputs": [],
   "source": [
    "result1 = agent(\"What are side effects of kidney cancer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g62ATwUE9Cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVgN65XXbQSd"
   },
   "source": [
    "### **Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AptIoWytAsLX",
    "outputId": "5fc43c19-eabc-48d7-b13d-a36e7bac39de"
   },
   "outputs": [],
   "source": [
    "llm_chain(\"How to prepare eggs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9LO2pgqEpdv",
    "outputId": "39e116dc-615a-445b-e6ab-8c5c23479897"
   },
   "outputs": [],
   "source": [
    "llm_chain(\"How to start a car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qlZ33JaGYvf"
   },
   "outputs": [],
   "source": [
    "template2 = \"\"\"Question: /n {question}. Answer: \"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template=template2,\n",
    "    input_variables= [\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4t3kxq9GqK6"
   },
   "outputs": [],
   "source": [
    "llm_chain_2 = LLMChain(prompt=prompt2, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKKWDk8SHpVE"
   },
   "outputs": [],
   "source": [
    "result_explanation = llm_chain_2(\"Explain antibiotics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "2S8xce-eIybg",
    "outputId": "962cf2bf-d734-49d1-b100-8b468d4c6834"
   },
   "outputs": [],
   "source": [
    "result_explanation['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g8Fujv3JcNT"
   },
   "outputs": [],
   "source": [
    "\"\"\"Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.\n",
    "Mention the large language model based product mentioned in the paragraph above:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7u2Ej_QKLDX",
    "outputId": "46a4a21e-efbe-499d-8068-5012f5bf66ee"
   },
   "outputs": [],
   "source": [
    "prompt_pt_grafos = \"\"\"No ramo de análise de grafos, existe uma métrica chamada Clustering Coefficient,\n",
    "   você pode me falar como interpretar ela?\"\"\"\n",
    "llm_chain_2(prompt_pt_grafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oK3Z2vgKjQu",
    "outputId": "eef92e52-ee04-4f00-a0b3-086ceccc9b7d"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"what is a convolution?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoJr-OeRK5vH"
   },
   "outputs": [],
   "source": [
    "prompt_code = \"\"\" I have to pass 2 values that are as a string\n",
    "\n",
    "'2,3'\n",
    "\n",
    "And turn them into a tuple\n",
    "\n",
    "(2,3), how to do?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF7gCSpWL_o9"
   },
   "source": [
    "(hit token limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Khc5-miyLGfS",
    "outputId": "94306cc3-2c44-400e-ac00-08c28a929db9"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(prompt_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lugOKzmJNmqe",
    "outputId": "89dbd883-826f-47c2-df96-f2e0fb3f2166"
   },
   "outputs": [],
   "source": [
    "prompt_code2 = \"\"\"\n",
    "/*\n",
    "Write a python code to ask the user for their name and say \"Hello\"\n",
    "*/\n",
    "\"\"\"\n",
    "llm_chain_2(prompt_code2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOAjMYTRZDOJ",
    "outputId": "92de7551-7c3a-45ca-9350-513ed03e6e34"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"How to convert a base64 file to bytes in python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZWmXRNcOkHu",
    "outputId": "b59d0146-b501-417f-bdb5-7e89852602d7"
   },
   "outputs": [],
   "source": [
    "prompt_sql = \"\"\"\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "Create a MySQL query for all students in the Computer Science Department\n",
    "\"\"\"\n",
    "llm_chain_2(prompt_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFdjN08kRpQA",
    "outputId": "5d8449cb-0724-4054-8a79-f4f44a1c6abb"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"como funciona o método __call__ em python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvYRShDwabYZ",
    "outputId": "a270dcc1-0841-4192-b178-366e3776f745"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"show me how python's args and kwargs work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LV2UWNBR-Ty",
    "outputId": "17737778-a4a8-4334-cfb0-205513bc6a5d"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"What's latency definition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Tvu7umqZh6B",
    "outputId": "b56519f1-739a-4027-b80e-a30c33b76b43"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"what is Python's ABC library and what is it for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLtxSdoPZsG7",
    "outputId": "7c5c80a5-15af-4ff6-8676-1347c8ad7004"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"Write me a diet, my goal is to gain lean mass and I will work out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UD8kpsgYxbM",
    "outputId": "f9c2ed73-f50b-4765-e89a-9d9940066335"
   },
   "outputs": [],
   "source": [
    "llm_chain_2(\"what is the difference between Similarity embeddings and search embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiBTopsPMYrM"
   },
   "source": [
    "Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdOzkJAAM0me"
   },
   "outputs": [],
   "source": [
    "template_chat = \"\"\"You are now a conversational assistant and must answer the questions: /n {history}\"\"\"\n",
    "\n",
    "prompt_chat = PromptTemplate(\n",
    "    template=template_chat,\n",
    "    input_variables= [\"history\"]\n",
    ")\n",
    "llm_chain_chat = LLMChain(prompt=prompt_chat, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jTmSnzzMG4L"
   },
   "outputs": [],
   "source": [
    "prompt_conversation1 = \"\"\"\n",
    "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
    "Human: Hello, who are you?\n",
    "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
    "Human: Can you tell me about the creation of blackholes?\n",
    "AI:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvx1F22mMjxt",
    "outputId": "21b56950-15d2-4e6f-c34e-25d58bf82223"
   },
   "outputs": [],
   "source": [
    "llm_chain_chat(prompt_conversation1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0635039fc1bf4e8385b610b2e821d337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "632d7836000748c2a9180971b0bb16cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64ab742910744170bd89317551c16983": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84a185a47e254158ae9f7f492ae02fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d308ecc2e23e4a378e9cc7ff5211dc3c",
       "IPY_MODEL_e10854f16f7c4e7faa88ca52c3b18fe1",
       "IPY_MODEL_b581a351180f4a1ea9248d71c457e5dd"
      ],
      "layout": "IPY_MODEL_64ab742910744170bd89317551c16983"
     }
    },
    "b581a351180f4a1ea9248d71c457e5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_632d7836000748c2a9180971b0bb16cb",
      "placeholder": "​",
      "style": "IPY_MODEL_b9b752bca85d4c67b94acce7799eb3b8",
      "value": " 1/15 [00:08&lt;02:04,  8.91s/it]"
     }
    },
    "b60bae131e7c4c598221192dd0bf9575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b9b752bca85d4c67b94acce7799eb3b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d308ecc2e23e4a378e9cc7ff5211dc3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0635039fc1bf4e8385b610b2e821d337",
      "placeholder": "​",
      "style": "IPY_MODEL_d8fdf0dbcf4c4f1c9fb6404849a3c1b0",
      "value": "Loading checkpoint shards:   7%"
     }
    },
    "d7076616114547709f55a2b6dadd5b47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8fdf0dbcf4c4f1c9fb6404849a3c1b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e10854f16f7c4e7faa88ca52c3b18fe1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7076616114547709f55a2b6dadd5b47",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b60bae131e7c4c598221192dd0bf9575",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
